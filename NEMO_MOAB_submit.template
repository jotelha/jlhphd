#!/bin/bash -x
#MSUB -E
#MSUB -v OMP_NUM_THREADS=1
#MSUB -l nodes=$${nodes}:ppn=$${ppn}
#MSUB -l walltime=$${walltime}
#MSUB -l pmem=5000mb
#MSUB -l partition=torque
#MSUB -o $${logdir}/$${job_name}.o$JOBID
#MSUB -e $${logdir}/$${job_name}.e$JOBID
#MSUB -m ae
###MSUB -M johannes.hoermann@imtek.uni-freiburg.de
#MSUB -N $${job_name}

# if possible, map tasks by socket
MAP_BY=socket
if [[ $((10 % $OMP_NUM_THREADS)) != 0 ]]; then
  echo "Allow tasks to distribute threads across different sockets, map by node."
  MAP_BY=node
fi

## check if $SCRIPT_FLAGS is "set"
if [ -n "${SCRIPT_FLAGS}" ] ; then
   ## but if positional parameters are already present
   ## we are going to ignore $SCRIPT_FLAGS
   if [ -z "${*}"  ] ; then
      set -- ${SCRIPT_FLAGS}
   fi
fi

# parse arguments
args=$(getopt -n "$0" -l "dry-run,no-mpi,forward-mpi-options,joblistfile:" -o "d1fj:" -- "$@")
eval set -- "$args"
JOBLISTFILE=joblistfile.txt
DRY_RUN=false
NO_MPI=false

echo "Got $# arguments: " $@
while true; do
  case "$1" in
    -d | --dry-run )             DRY_RUN=true; shift ;;
    -1 | --no-mpi )              NO_MPI=true; shift ;;
    -f | --forward-mpi-options ) FORWARD_MPI_OPTIONS=true; shift ;;
    -j | --joblistfile )         JOBLISTFILE="$2"; shift; shift ;;
    -- ) shift; break ;;
    * ) break ;;
  esac
done


# only defined for local use, not used by other executables:
TASK_COUNT=$((${MOAB_PROCCOUNT}/${OMP_NUM_THREADS}))
MPI_PPN_COUNT=$((${PBS_NUM_PPN}/${OMP_NUM_THREADS}))
MPIRUN_OPTIONS="--bind-to core --map-by $MAP_BY:PE=${OMP_NUM_THREADS}"
MPIRUN_OPTIONS="${MPIRUN_OPTIONS} -n ${TASK_COUNT} --report-bindings"
# source: https://software.intel.com/en-us/get-started-with-mpi-for-linux
# mpirun -n <# of processes> -ppn <# of processes per node> ./myprog

# source: https://software.intel.com/en-us/node/535581
# In the command line above:
#    -n sets the number of MPI processes to launch; if the option is not
#        specified, the process manager pulls the host list from a job
#        scheduler, or uses the number of cores on the machine.
#    -ppn sets the number of processes to launch on each node; if the option is
#        not specified, processes are assigned to the physical cores on the
#        first node; if the number of cores is exceeded, the next node is used.
#    -f specifies the path to the host file listing the cluster nodes;
#        alternatively, you can use the -hosts option to specify a
#        comma-separated list of nodes; if hosts are not specified, the local
#        node is used.
#    myprog is the name of your MPI program.

# source: https://software.intel.com/en-us/node/589998
#  -binding "<parameter>=<value>[;<parameter>=<value> ...]"
#   <value> can be omp:<layout> with
#     omp - domain size = OMP_NUM_THREADS environment variable value
#   and <layout>
#     Each member location inside the domain is defined by the optional
#     <layout> parameter value:
#       compact - as close with others as possible in the multi-core topology
#       scatter - as far away from others as possible in the multi-core topology
#       range - by BIOS numbering of the processors
#     If <layout> parameter is omitted, compact is assumed as the value of
#     <layout>

# GROMACS warning:
# Your choice of number of MPI ranks and amount of resources results in using 10
# OpenMP threads per rank, which is most likely inefficient. The optimum is
# usually between 1 and 6 threads per rank. If you want to run with this setup,
# specify the -ntomp option. But we suggest to change the number of MPI ranks.

# Source: https://www.bwhpc-c5.de/wiki/index.php/Batch_Jobs#Multithreaded_.2B_MPI_parallel_Programs
#  1.3.2.4 Multithreaded + MPI parallel Programs
# Multithreaded + MPI parallel programs operate faster than serial programs on
# multi CPUs with multiple cores. All threads of one process share resources
# such as memory. On the contrary MPI tasks do not share memory but can be
# spawned over different nodes.
# Multiple MPI tasks using OpenMPI must be launched by the MPI parallel program
# mpirun. For multithreaded programs based on Open Multi-Processing (OpenMP)
# number of threads are defined by the environment variable OMP_NUM_THREADS.
# By default this variable is set to 1 (OMP_NUM_THREADS=1).

# export KMP_AFFINITY=scatter
# GMXNOTE: KMP_AFFINITY set, will turn off gmx mdrun internal affinity
#      setting as the two can conflict and cause performance degradation.
#      To keep using the gmx mdrun internal affinity setting, unset the
#      KMP_AFFINITY environment variable or set it to 'none' or 'disabled'.
# export KMP_AFFINITY=none

export MOAB_NODECOUNT=$PBS_NUM_NODES

# pmem is required memory per task

printf '%*s\n' "${COLUMNS:-$(tput cols)}" '' | tr ' ' -
printenv

printf '%*s\n' "${COLUMNS:-$(tput cols)}" '' | tr ' ' -

echo -e "job id \t\t ${MOAB_JOBID}"
echo -e "job name \t ${MOAB_JOBNAME}"
echo -e "#nodes \t\t ${MOAB_NODECOUNT}" # usually empty
echo -e "#nodes (pbs)\t ${PBS_NUM_NODES}" # reliable
echo -e "#cores \t\t ${MOAB_PROCCOUNT}" # total number of cores
echo -e "#tasks \t\t ${TASK_COUNT}"
echo -e "#tasks (pbs)\t ${PBS_TASKNUM}" # just 1
echo -e "#threads \t ${OMP_NUM_THREADS}"
echo -e "#ppn \t\t ${PBS_NUM_PPN}"
echo -e "nodes \t\t ${MOAB_NODELIST}"
echo -e "pbs dir \t ${PBS_O_WORKDIR}"

#echo -e "out dir \t ${DIRECTORY}"

printf '%*s\n' "${COLUMNS:-$(tput cols)}" '' | tr ' ' -

# cd ${MOAB_SUBMITDIR} does not work for array jobs, but PBS_O_WORKDIR does
module use /work/ws/nemo/fr_lp1029-IMTEK_SIMULATION-0/modulefiles
module use /home/fr/fr_fr/fr_jh1130/modulefiles

module load devel/python/3.6-local
module load mdtools
module load gromacs/2018.1-gnu-5.2

$${pre_rocket}
cd $${launch_dir}
echo "Current directory: $(pwd)"

$${rocket_launch}
$${post_rocket}

printf '%*s\n' "${COLUMNS:-$(tput cols)}" '' | tr ' ' -
